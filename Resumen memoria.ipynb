{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importan las librerías a ocupar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.text_cell_render p, .text_cell_render li { font-size: 13pt !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.text_cell_render li { margin: 0 0 10px 0; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# \n",
    "import datetime\n",
    "from functools import reduce\n",
    "import math\n",
    "from keras.models import model_from_json\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Tensorflow GPU config\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Keras libs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout, TimeDistributed, CuDNNGRU, GRU\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Matplotlib setting\n",
    "def set_chart_font():\n",
    "    font = {'weight' : 'normal', 'size'   : 16}\n",
    "    import matplotlib\n",
    "    matplotlib.rc('font', **font)\n",
    "\n",
    "# Jupyter stuff\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "display(HTML(\"<style>.text_cell_render p, .text_cell_render li { font-size: 13pt !important; }</style>\"))\n",
    "display(HTML(\"<style>.text_cell_render li { margin: 0 0 10px 0; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el dataset con las mediciones del interior y exterior de la casa unificadas. Se descartan las variables co2, ruido, dispositivo_id y measured_at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/mediciones_unificadas.csv\")\n",
    "df.date_measured_at = pd.to_datetime(df.date_measured_at)\n",
    "df = df.drop(columns=['co2', 'ruido', 'dispositivo_id', 'measured_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se analiza el estado de las series de tiempo para determinar conjunto de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_OF_TIME_SERIES = {}\n",
    "TARGET_DF = df\n",
    "\n",
    "for i, g in TARGET_DF.groupby('vivienda_id'):\n",
    "    HASH_OF_TIME_SERIES[i] = g.set_index('date_measured_at')\n",
    "\n",
    "def show_stats(seried_df):\n",
    "    print(\"Número de registros:\", seried_df.shape[0])\n",
    "    temp = seried_df.resample('30min').mean()\n",
    "    missing_rate = (temp.isna().sum()/len(temp) * 100).temperatura_interior\n",
    "    print(\"Porcentaje de información faltante:\", missing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de casas con las que entrenar: 89\n",
      "Total casas: 303\n"
     ]
    }
   ],
   "source": [
    "houses_stats = []\n",
    "# Se define conjunto de entrenamiento\n",
    "training_houses_pool = []\n",
    "for i in HASH_OF_TIME_SERIES.keys():\n",
    "    temp = HASH_OF_TIME_SERIES[i].resample('30min').mean()\n",
    "    missing_rate = (temp.isna().sum()/len(temp) * 100).temperatura_interior\n",
    "    \n",
    "    # Que le falte 10% o menos y que tenga 6 meses de medición\n",
    "    if missing_rate <= 10 and HASH_OF_TIME_SERIES[i].shape[0] > 8000:\n",
    "        training_houses_pool.append(i)\n",
    "        # print(\"Casa {}, Porcentage de null: {}%\".format(i, missing_rate))\n",
    "        houses_stats.append( (i, HASH_OF_TIME_SERIES[i].shape[0], missing_rate) )\n",
    "\n",
    "# Se define el conjunto de validación\n",
    "validation_houses_pool = set(HASH_OF_TIME_SERIES.keys()) - set(training_houses_pool)\n",
    "        \n",
    "print(\"Cantidad de casas con las que entrenar:\", len(training_houses_pool))\n",
    "print(\"Total casas:\", len(HASH_OF_TIME_SERIES.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las siguientes funciones para realizar todo el procedimiento asociado de tratamiento de datos y entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_A(number_of_examples):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Recurrente\n",
    "    model.add(CuDNNGRU(input_shape=(number_of_examples, 4), units=100, return_sequences=False))\n",
    "\n",
    "    # FF\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')#, metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "    return model\n",
    "\n",
    "def generate_training_houses_set(array_of_houses, training_percentage):\n",
    "    amount_of_houses = len(array_of_houses)\n",
    "    train_lenght = int(amount_of_houses * training_percentage)\n",
    "    training_houses = set(np.random.choice(array_of_houses, train_lenght, replace=False))\n",
    "    return training_houses\n",
    "\n",
    "# Función que recibe una serie de pandas y retorna un robust scaler\n",
    "def initialize_robust_scalers(series_df):\n",
    "    scalers_dictionary = dict()\n",
    "    \n",
    "    # Se pone un RobustScaler por cada feature\n",
    "    for i in FEATURES_IN_STUDY:\n",
    "        scalers_dictionary[i] = RobustScaler()\n",
    "    \n",
    "    for column_name in FEATURES_IN_STUDY:\n",
    "        # Se le hace fit al scaler\n",
    "        scalers_dictionary[column_name].fit( np.expand_dims(series_df[column_name], axis=1))\n",
    "\n",
    "    return scalers_dictionary\n",
    "\n",
    "# Recibe una serie de pandas y un diccionario de RobustScalers. Aplica los scalers a la serie.\n",
    "def standarize_serie(serie_df, scalers_dictionary):\n",
    "    copy = serie_df.copy()\n",
    "    for column_name in FEATURES_IN_STUDY:\n",
    "        # Se transforma la data\n",
    "        copy[column_name] = scalers_dictionary[column_name].transform(np.expand_dims(copy[column_name], axis=1))\n",
    "        \n",
    "    return copy\n",
    "\n",
    "# Recibe una serie (standarizada) y retorna la matriz laggeada\n",
    "def generate_lagged_matrix(serie_df, amoung_of_lag):\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    last_seen = list()\n",
    "    \n",
    "    for index, row in serie_df.iterrows():\n",
    "        current_record = list()\n",
    "        for i in FEATURES_IN_STUDY:\n",
    "            current_record.append(row[i])\n",
    "        if len(last_seen) < amoung_of_lag:\n",
    "            last_seen.append(current_record)\n",
    "        else:\n",
    "            Y.append(list(current_record))\n",
    "            X.append(list(last_seen))\n",
    "            del last_seen[0]\n",
    "            last_seen.append(current_record)\n",
    "            \n",
    "    return X, Y\n",
    "\n",
    "def generate_training_matrix(amount_of_lag, training_ids, general_df, hash_of_time_series):\n",
    "    # Se proceden a inicializar los RobustScalers\n",
    "    training_mask = general_df['vivienda_id'].map(lambda x: x in training_ids)\n",
    "    scalers = initialize_robust_scalers(general_df[training_mask])\n",
    "    \n",
    "    standarized_time_series = dict()\n",
    "    X_train, Y_train = [], []\n",
    "    # training_matrixs = dict()\n",
    "    for i in training_ids:\n",
    "        standarized_time_series[i] = standarize_serie(hash_of_time_series[i], scalers)\n",
    "        x, y = generate_lagged_matrix(standarized_time_series[i], amount_of_lag)\n",
    "        X_train.append(x)\n",
    "        Y_train.append(y)\n",
    "        \n",
    "    X_train = np.asarray(reduce(lambda x,y: x+y, X_train))\n",
    "    Y_train = np.asarray(reduce(lambda x,y: x+y, Y_train))\n",
    "    \n",
    "    return X_train, Y_train, scalers\n",
    "\n",
    "def generate_validation_matrix(amount_of_lag, scalers, validation_series):\n",
    "    validation_hash = dict()\n",
    "    for i in validation_series:\n",
    "        standarized_serie = standarize_serie(validation_series[i], scalers)\n",
    "        x, y = generate_lagged_matrix(standarized_serie, amount_of_lag)\n",
    "        x, y = np.asarray(x), np.asarray(y)\n",
    "        validation_hash[i] = (x, y)\n",
    "        \n",
    "    return validation_hash\n",
    "\n",
    "def train_model(model, X_train, Y_train):\n",
    "    history = model.fit(X_train, Y_train, epochs=8, batch_size=256, verbose=1) \n",
    "    return history\n",
    "\n",
    "# Recibe un iterable con los ids de validación y el hash de todas la series\n",
    "# Retorna un hash con las series. En cada entrada está la serie continua más larga\n",
    "def clean_validation_data(validation_ids, hash_of_time_series):\n",
    "    validation_series = {}\n",
    "    for i in validation_ids:\n",
    "        if hash_of_time_series[i].shape[0] >= 4000:\n",
    "            resampled_house = hash_of_time_series[i].resample('30min').mean()\n",
    "            initial_date=\"\"\n",
    "            best_initial_date = \"\"\n",
    "            best_end_date = \"\"\n",
    "            longest_days = 0\n",
    "\n",
    "            for index, row in resampled_house.iterrows():\n",
    "\n",
    "                temperature = getattr(row, 'temperatura_interior')\n",
    "                current_date = index\n",
    "\n",
    "                # Si es que si veo algo de temperatura\n",
    "                if not math.isnan(temperature):  \n",
    "                    if initial_date == \"\":\n",
    "                        initial_date = current_date\n",
    "\n",
    "                else:\n",
    "                    if initial_date != \"\":\n",
    "                        delta = last_date - initial_date\n",
    "                        if delta.days > longest_days:\n",
    "                            longest_days = delta.days\n",
    "                            best_initial_date = initial_date\n",
    "                            best_end_date = last_date\n",
    "                        initial_date = \"\"\n",
    "\n",
    "                last_date = current_date\n",
    "\n",
    "            if best_end_date!=\"\":\n",
    "                temp = hash_of_time_series[i][best_initial_date:best_end_date]\n",
    "\n",
    "                # Como mínimo 4 meses de medición\n",
    "                if temp.shape[0] >= 4000:\n",
    "                    validation_series[i] = hash_of_time_series[i][best_initial_date:best_end_date]\n",
    "        \n",
    "    return validation_series\n",
    "\n",
    "# Convierte una matriz (m x 4) a una de (m x 1) en donde el 1 es el feature seleccionado\n",
    "# en la escala original c:\n",
    "def return_feature_to_original_scale(matrix, scalers, feature):\n",
    "    if feature == 0:\n",
    "        scaler = scalers['temperatura_interior']\n",
    "    elif feature == 1:\n",
    "        scaler = scalers['humedad_interior']\n",
    "    elif feature == 2:\n",
    "        scaler = scalers['temperatura_exterior']\n",
    "    else:\n",
    "        scaler = scalers['humedad_exterior'] \n",
    "    \n",
    "    transformed = np.expand_dims(matrix[:,feature], axis=1)\n",
    "    return scaler.inverse_transform(transformed)\n",
    "\n",
    "# Calcula las metricas dado el modelo y el hash de series de validacion\n",
    "# Calcula el mse general y el mae por atributo\n",
    "def calculate_metrics(model, scalers, amount_of_lag, hash_of_matrix):\n",
    "    \n",
    "    # Calcula la media de una lista\n",
    "    def calculate_mean(l):\n",
    "        return sum(l)/len(l)\n",
    "\n",
    "    in_temperature_mae = list()\n",
    "    in_hum_mae = list()\n",
    "    out_temperature_mae = list()\n",
    "    out_hum_mae = list()\n",
    "    \n",
    "    total_mse = list()\n",
    "    \n",
    "    for i in hash_of_matrix:\n",
    "        X_val, Y_val = hash_of_matrix[i]\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        total_mse.append(metrics.mean_squared_error(Y_val, y_pred))\n",
    "        \n",
    "        for j in range(4):\n",
    "            y_pred_transformed = return_feature_to_original_scale(y_pred, scalers, j)\n",
    "            y_transformed = return_feature_to_original_scale(Y_val, scalers, j)\n",
    "\n",
    "            # mse = metrics.mean_squared_error(y_transformed, y_pred_transformed)\n",
    "            mae = metrics.mean_absolute_error(y_transformed, y_pred_transformed)\n",
    "            \n",
    "            if j == 0:\n",
    "                in_temperature_mae.append(mae)\n",
    "            elif j == 1:\n",
    "                in_hum_mae.append(mae)\n",
    "            elif j == 2:\n",
    "                out_temperature_mae.append(mae)\n",
    "            else:\n",
    "                out_hum_mae.append(mae)\n",
    "        \n",
    "    return ( calculate_mean(total_mse), calculate_mean(in_temperature_mae), calculate_mean(in_hum_mae), calculate_mean(out_temperature_mae) , calculate_mean(out_hum_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_series_hash = clean_validation_data(validation_houses_pool, HASH_OF_TIME_SERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1032105/1032105 [==============================] - 11s 11us/step - loss: 0.0048\n",
      "Epoch 2/8\n",
      "1032105/1032105 [==============================] - 10s 10us/step - loss: 0.0031\n",
      "Epoch 3/8\n",
      "1032105/1032105 [==============================] - 11s 11us/step - loss: 0.0031\n",
      "Epoch 4/8\n",
      "1032105/1032105 [==============================] - 12s 11us/step - loss: 0.0030\n",
      "Epoch 5/8\n",
      "1032105/1032105 [==============================] - 11s 11us/step - loss: 0.0030\n",
      "Epoch 6/8\n",
      "1032105/1032105 [==============================] - 11s 10us/step - loss: 0.0030\n",
      "Epoch 7/8\n",
      "1032105/1032105 [==============================] - 11s 10us/step - loss: 0.0030\n",
      "Epoch 8/8\n",
      "1032105/1032105 [==============================] - 11s 10us/step - loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "def generate_network_A(input_length):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Recurrente\n",
    "    model.add(CuDNNGRU(input_shape=(input_length, 4), units=100, return_sequences=False))\n",
    "\n",
    "    # FF\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(4))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')#, metrics=['mean_absolute_error', 'mean_absolute_percentage_error'])\n",
    "    return model\n",
    "\n",
    "FEATURES_IN_STUDY = [\"temperatura_interior\", \"humedad_interior\", \"temperatura_exterior\", \"humedad_exterior\"]\n",
    "LAG_30_MINUTES = 4\n",
    "\n",
    "model_30minutes = generate_network_A(LAG_30_MINUTES)\n",
    "ids_30minutes = generate_training_houses_set(training_houses_pool, 0.5)\n",
    "X_train, Y_train, scalers_30minutes = generate_training_matrix(LAG_30_MINUTES, ids_30minutes, df, HASH_OF_TIME_SERIES)\n",
    "hash_of_validation_matrix_30minutes = generate_validation_matrix(LAG_30_MINUTES, scalers_30minutes, validation_series_hash)\n",
    "\n",
    "train_model(model_30minutes, X_train, Y_train)\n",
    "model_30minutes_metrics = calculate_metrics(model_30minutes, scalers_30minutes, LAG_30_MINUTES, hash_of_validation_matrix_30minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE Temperatura interior</th>\n",
       "      <th>MAE Humedad interior</th>\n",
       "      <th>MAE Temperatura exterior</th>\n",
       "      <th>MAE Humedad exterior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.142847</td>\n",
       "      <td>0.492806</td>\n",
       "      <td>0.209711</td>\n",
       "      <td>0.906668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSE  MAE Temperatura interior  MAE Humedad interior  \\\n",
       "0  0.002614                  0.142847              0.492806   \n",
       "\n",
       "   MAE Temperatura exterior  MAE Humedad exterior  \n",
       "0                  0.209711              0.906668  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_table_metrics(metrics):\n",
    "    data = {'MSE':  [metrics[0]],\n",
    "            'MAE Temperatura interior':  [metrics[1]],\n",
    "            'MAE Humedad interior':  [metrics[2]],\n",
    "            'MAE Temperatura exterior':  [metrics[3]],\n",
    "            'MAE Humedad exterior':  [metrics[4]]}\n",
    "    display(pd.DataFrame(data, columns = ['MSE','MAE Temperatura interior', 'MAE Humedad interior', 'MAE Temperatura exterior', 'MAE Humedad exterior']))\n",
    "    \n",
    "show_table_metrics(model_30minutes_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a guardar la red obtenida. Se guardan 3 archivos, el modelo va en `model_30minutes.json`, los pesos del modelo van en `model_30minutes.h5` y finalmente los escaladores van en `scalers_30minutes.save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalers_30minutes.save']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"model_30minutes.json\", \"w\") as json_file:\n",
    "    json_file.write(model_30minutes.to_json())\n",
    "    \n",
    "model_30minutes.save_weights(\"model_30minutes.h5\")\n",
    "\n",
    "\n",
    "scaler_filename = \"scalers_30minutes.save\"\n",
    "joblib.dump(scalers_30minutes, \"scalers_30minutes.save\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero hay que cargar la RNA, los pesos y los escaladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_table_metrics(metrics):\n",
    "    data = {'MSE':  [metrics[0]],\n",
    "            'MAE Temperatura interior':  [metrics[1]],\n",
    "            'MAE Humedad interior':  [metrics[2]],\n",
    "            'MAE Temperatura exterior':  [metrics[3]],\n",
    "            'MAE Humedad exterior':  [metrics[4]]}\n",
    "    display(pd.DataFrame(data, columns = ['MSE','MAE Temperatura interior', 'MAE Humedad interior', 'MAE Temperatura exterior', 'MAE Humedad exterior']))\n",
    "\n",
    "# Se cargan los escaladores\n",
    "loaded_scarler = joblib.load(\"scalers_30minutes.save\") \n",
    "\n",
    "# Se carga y crea el modelo\n",
    "with open(\"model_30minutes.json\", \"r\") as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Se cargan los pesos al modelo\n",
    "loaded_model.load_weights(\"model_30minutes.h5\")\n",
    "\n",
    "# Se corrobora que el rendimiento sea el esperado\n",
    "# show_table_metrics(calculate_metrics(loaded_model, loaded_scarler, LAG_30_MINUTES, hash_of_validation_matrix_30minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después es necesario tener un input a predecir, en este caso se utiliza la variable `example_data` a modo de ejemplo. Simplemente se tiene que invocar el método generate prediction y retornará la predicción segun corresponda el modelo.\n",
    "\n",
    "Importante: Notar que como el modelo es de 30 minutos se trabaja con las últimas 4 mediciones, en el caso de 1 hora se debe invocar con 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.643192, 71.95427 , 12.886521, 90.59765 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sacando datos de ejemplo\n",
    "# display(validation_series_hash[2][:5])\n",
    "\n",
    "example_data = [ [17.4, 74, 13.2, 88], [17.7, 73, 13.1, 89], [17.7, 73, 13.1, 89], [17.7, 72, 13.0, 90]]\n",
    "\n",
    "def generate_prediction(model, scalers, input_array):\n",
    "    DICT_FEATURES_IN_STUDY = {0:\"temperatura_interior\", 1:\"humedad_interior\", 2:\"temperatura_exterior\", 3:\"humedad_exterior\"}\n",
    "    transformed_input = list()\n",
    "    for i in input_array:\n",
    "        transformed_data = list()\n",
    "        for j in range(len(i)):\n",
    "            # Se transforma la data\n",
    "            transformed_data.append(scalers[DICT_FEATURES_IN_STUDY[j]].transform(np.expand_dims([i[j]], axis=1))[0][0])\n",
    "        transformed_input.append(transformed_data)\n",
    "    transformed_input = np.asarray([transformed_input])\n",
    "    predicted_value = model.predict(transformed_input)\n",
    "    \n",
    "    # Se tienen que devolver los datos a la escala original\n",
    "    final_answer = list()\n",
    "    for i in range(4):\n",
    "        a = np.expand_dims(predicted_value[:,i], axis=1)\n",
    "        final_answer.append(scalers[DICT_FEATURES_IN_STUDY[i]].inverse_transform(a))\n",
    "        \n",
    "    return np.asarray(final_answer).flatten()\n",
    "\n",
    "generate_prediction(loaded_model, loaded_scarler, example_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
